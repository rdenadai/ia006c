{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "import uuid\n",
    "from unicodedata import normalize\n",
    "from string import punctuation\n",
    "import warnings\n",
    "from multiprocessing import cpu_count\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import machado, mac_morpho, floresta\n",
    "from nltk.cluster import KMeansClusterer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance, euclidean_distance\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, silhouette_score, davies_bouldin_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.manifold import MDS, TSNE, LocallyLinearEmbedding\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import DBSCAN, OPTICS, KMeans\n",
    "from pyclustering.cluster.xmeans import xmeans, kmeans\n",
    "from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n",
    "from pyclustering.cluster.elbow import elbow\n",
    "from pyclustering.cluster.silhouette import silhouette_ksearch_type, silhouette_ksearch\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from IPython.display import HTML\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123456)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_acentos(txt):\n",
    "    return normalize('NFKD', txt).encode('ASCII', 'ignore').decode('ASCII')\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        complex(s) # for int, long, float and complex\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def tokenizer(phrase):\n",
    "    phrase = phrase.lower()\n",
    "    for o, r in RM:\n",
    "        phrase = re.sub(o, r, phrase, flags=re.MULTILINE)\n",
    "    phrase = re.sub(r'[\"\\'@#%\\(\\)]', \" \", phrase)\n",
    "    phrase = NLP(re.sub(r'[-+=:]', \"\", phrase), disable=[\"parser\"])\n",
    "    clean_frase = []\n",
    "    for palavra in phrase:\n",
    "        if palavra.pos_ != \"PUNCT\":\n",
    "            word = palavra.text.strip()\n",
    "            if not is_number(word) and word not in STOPWORDS and len(word) > 1:\n",
    "                # clean_frase += [STEMMER.stem(remover_acentos(palavra.lemma_))]\n",
    "                clean_frase += [remover_acentos(palavra.lemma_)]\n",
    "    return clean_frase\n",
    "\n",
    "def _get_stopwords():\n",
    "    stpwords = stopwords.words('portuguese') + list(punctuation)\n",
    "    rms = [\"um\", \"uma\", \"n√£o\", \"mais\", \"muito\"]\n",
    "    for rm in rms:\n",
    "        del stpwords[stpwords.index(rm)]\n",
    "    return stpwords\n",
    "\n",
    "NLP = spacy.load(\"pt\")\n",
    "# STEMMER = nltk.stem.RSLPStemmer()\n",
    "STEMMER = nltk.stem.SnowballStemmer('portuguese')\n",
    "STOPWORDS = _get_stopwords()\n",
    "RM = [\n",
    "    ('\\n', '. '), ('\"', ''), ('@', ''),\n",
    "    ('#', ''), ('RT', ''), (r'(http[s]*?:\\/\\/)+.*[\\r\\n]*', '')\n",
    "]\n",
    "\n",
    "colors = np.asarray([\n",
    "    (230, 25, 75), (60, 180, 75), (255, 225, 25), (0, 130, 200), (245, 130, 48), (145, 30, 180), (70, 240, 240),\n",
    "    (240, 50, 230), (210, 245, 60), (250, 190, 190), (0, 128, 128), (230, 190, 255), (170, 110, 40), (255, 250, 200),\n",
    "    (128, 0, 0), (170, 255, 195), (128, 128, 0), (255, 215, 180), (0, 0, 128), (128, 128, 128), (0, 0, 0),\n",
    "    (141, 204, 226), (212, 50, 147), (251, 226, 93), (16, 185, 176), (189, 182, 163), (244, 172, 18), (100, 28, 112),\n",
    "    (165, 124, 92), (183, 160, 211), (66, 47, 105), (240, 67, 7) # , (244, 188, 198)\n",
    "]).astype(np.float32)\n",
    "\n",
    "colors /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin = 5\n",
    "kmax = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Confira o prov√°vel setlist do show Amigos</td>\n",
       "      <td>Na noite deste s√°bado, na Arena do Gr√™mio, Chit√£ozinho &amp; Xoror√≥, Zez√© di Camargo &amp; Luciano e Leonardo apresentam o show Amigos 20 Anos - A Hist√≥ria Continua, que faz um passeio nos sucessos dos sertanejos. O primeiro show da turn√™ foi em julho, em Be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cr√≠tica | A Grande Mentira (2019): verdadeiros atores</td>\n",
       "      <td>Dois idosos se conhecem num site para encontros rom√¢nticos. Ao se encontrarem, revelam que mentiram sobre seus nomes. Isso acaba quebrando o gelo entre o casal e d√° in√≠cio a um relacionamento de intimidade de tal n√≠vel que a rec√©m-vi√∫va n√£o hesita em hosped√°-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>F√£ cria arte da vers√£o sombria da Capit√£ Marvel</td>\n",
       "      <td>O artista BossLogic decidiu alterar suas obras com Brie Larson para criar a vers√£o ‚Äòlive action‚Äô da Capit√£ Marvel que est√° matando os Vingadores nos quadrinhos. E o resultado pode ser visto abaixo:Reworked one of my #captainmarvel pieces, made it a little d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Google vai matar o Cloud Print e d√° um ano aos utilizadores para encontrarem alternativa</td>\n",
       "      <td>Muitos n√£o perceberam a utilidade de ter um servi√ßo cloud para imprimir remotamente. Alguns guias de utiliza√ß√£o mostraram o poder deste servi√ßo gratuito da Google. Contudo, nem todos os servi√ßos conseguem vingar. Assim, num ano em que houve a morte do Inbox,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Guedes defende zerar INSS para empres√°rios, mas n√£o diz quem pagar√° a conta</td>\n",
       "      <td>Foto: Pedro Ladeira/FolhapressO ministro da Economia, Paulo Guedes, afirmou que √© poss√≠vel criar milh√µes de empregos se os encargos trabalhistas forem zerados, em um evento empresarial nesta sexta (22), no Rio de Janeiro, de acordo o jornal Valor Econ√¥m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>MacMagazine no Ar #347: evento em 2/12, teclado do MacBook Pro de 16‚Ä≥, AirPods Pro vs. Galaxy Buds, Donald Trump e mais!</td>\n",
       "      <td>E hoje √© dia de MacMagazine no Ar! üòÄ Esta √© a 347¬™ edi√ß√£o do nosso podcast.PodPesquisa 2019A Associa√ß√£o Brasileira de Podcasters (ABPod) est√° realizando a PodPesquisa 2019, a fim de desenhar o perfil do ouvinte de podcast brasileiro.Ela ficar√° no ar at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>PCP marca congresso para 27, 28 e 29 de novembro de 2020</td>\n",
       "      <td>O secret√°rio-geral do PCP, Jer√≥nimo de Sousa, anunciou hoje, em Lisboa, que vai marcar o XXI congresso do partido para 27, 28 e 29 de novembro de 2020, sustentando que o pa√≠s mant√©m os mesmos graves problemas estruturais.Jer√≥nimo de Sousa falava numa conf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Palcos da semana</td>\n",
       "      <td>Os pr√≥ximos dias trazem Gregorio Duvivier num S√≠sifo hiperligado, o regresso dos Vampire Weekend, os finalistas ao Pr√©mio Sonae Media Art, Akram Khan em dan√ßas de guerra e o comunismo em palco com Wen Hui e Jana Svobodov√°.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Pressionado, Facebook avalia mudan√ßas em regras para an√∫ncios pol√≠ticos</td>\n",
       "      <td>RIO Ap√≥s as decis√µes de GoogleeTwitter, de restringir osan√∫ncios pol√≠ticos, oFacebookest√° considerando altera√ß√µes em suas regras, informaram fontes ao Wall Street Journal. Segundo elas, a companhia est√° pensando em aumentar o n√∫mero m√≠nimo de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Protocolo de seguran√ßa, limpeza espiritual e at√© manual para a fam√≠lia: os preparativos dos rubro-negros para a final da Libertadores</td>\n",
       "      <td>√Äs v√©speras da decis√£o mais importante dos √∫ltimos 38 anos, todo cuidado √© pouco. Esse tem sido o mantra dos rubro-negros, que n√£o economizam em simpatias, correntes de WhatsApp e supersti√ß√µes para ver o Flamengo conquistar o bi da Libertadores. Entre as br</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"gnews.csv\", sep=';', names=[\"title\", \"description\"])\n",
    "df_data.dropna(inplace=True)\n",
    "\n",
    "T = df_data.to_numpy()\n",
    "index = np.random.choice(T.shape[0], 10, replace=False)\n",
    "display(HTML(df_data.iloc[index].sort_values(by=[\"title\"]).to_html(index=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization...\n",
      "Qtd documentos treino:  3074\n",
      "Finished...\n"
     ]
    }
   ],
   "source": [
    "def doc2token(doc):\n",
    "    if isinstance(doc, list):\n",
    "        doc = \" \".join(doc)\n",
    "    return TaggedDocument(tokenizer(doc), [str(uuid.uuid4())])\n",
    "\n",
    "print(\"Tokenization...\")\n",
    "docs = set(df_data[\"title\"])\n",
    "with ProcessPoolExecutor(max_workers=cpu_count()) as exc:\n",
    "    documents_d2c = list(exc.map(doc2token, docs, chunksize=100))\n",
    "print(\"Qtd documentos treino: \", len(documents_d2c))\n",
    "print(\"Finished...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['brasil', 'deter', 'mercar', 'mundial', 'niobio'], tags=['8d05e45a-f86d-48f8-8a4e-6cd537d22d65']),\n",
       " TaggedDocument(words=['philip', 'roth', 'vidar', 'umar', 'derrotar', 'perpetuo'], tags=['811e1d8c-6ef5-43f8-8131-6a578b382eb9']),\n",
       " TaggedDocument(words=['apresentacao', 'natal', 'livrar', 'sonho', 'conservatorio', 'belo', 'arte', 'acontecer', 'fim', 'semana', 'joinville'], tags=['df0db5be-d4aa-4280-bafc-7b99f0c5b644']),\n",
       " TaggedDocument(words=['novo', 'anuncio', 'jbc', 'spinoff', 'the', 'seven', 'deadly', 'sim', 'isekai', 'slime', 'coyote'], tags=['967f2a77-4144-411b-8566-8146e54851ca']),\n",
       " TaggedDocument(words=['importancia', 'felicidade', 'organizacao'], tags=['85bd4586-5647-4434-8343-c1d68660b6df']),\n",
       " TaggedDocument(words=['algoritmo', 'apps', 'paquera', 'tentar', 'prever', 'parir', 'perfeito'], tags=['cd7c24fb-6c01-4501-a58b-0f9c1755d635']),\n",
       " TaggedDocument(words=['xiaomi', 'dever', 'ampliar', 'esforco', 'focar', 'europa', 'dizer', 'executivo'], tags=['63b7962f-046d-4d62-9ed7-d0800823ff64']),\n",
       " TaggedDocument(words=['mi', 'mix', 'dever', 'ganhar', 'breve', 'dois', 'variante', 'turbinado', 'revelar', 'registro', 'tenaa'], tags=['48fc6651-2775-48c7-aa28-407d7d7d24f7']),\n",
       " TaggedDocument(words=['umar', 'foto', 'sensual', 'salvar', 'morte', 'um', 'pilotar', 'guerra', 'americano'], tags=['3fcc8496-8cac-4d4f-b9eb-82a5d86c7ec5']),\n",
       " TaggedDocument(words=['aventurar', 'juridico', 'licenca', 'matar', 'dizer', 'jurista', 'sobrar', 'excludente', 'ilicitude', 'projeto', 'bolsonaro'], tags=['8dcc9ccc-4579-43be-819e-2bd21e6cedc6'])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_d2c[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab...\n",
      "finish...\n"
     ]
    }
   ],
   "source": [
    "print(\"building vocab...\")\n",
    "vocab = copy.deepcopy(documents_d2c)\n",
    "# with ProcessPoolExecutor(max_workers=cpu_count()) as exc:\n",
    "#     print(\"Adding machado...\")\n",
    "#     vocab += list(exc.map(doc2token, utils.shuffle(machado.sents(), n_samples=4000), chunksize=100))\n",
    "#     print(\"Adding mac_morpho...\")\n",
    "#     vocab += list(exc.map(doc2token, utils.shuffle(mac_morpho.sents(), n_samples=4000), chunksize=100))\n",
    "#     print(\"Adding floresta...\")\n",
    "#     vocab += list(exc.map(doc2token, utils.shuffle(floresta.sents(), n_samples=4000), chunksize=100))\n",
    "print(\"finish...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3074"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions   : 750\n",
      "Epochs       : 200\n",
      "Infer Epochs : 15000\n"
     ]
    }
   ],
   "source": [
    "dim = 750\n",
    "epochs = 200\n",
    "M = 15000\n",
    "\n",
    "print(f\"Dimensions   : {dim}\")\n",
    "print(f\"Epochs       : {epochs}\")\n",
    "print(f\"Infer Epochs : {M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBOW: Building vocab...\n",
      "DBOW: Training...\n",
      "DMM: Building vocab...\n",
      "DMM: Training...\n",
      "Finish...\n"
     ]
    }
   ],
   "source": [
    "# print(\"Starting model...\")\n",
    "# common_kwargs = dict(\n",
    "#     dm=0, vector_size=dim, epochs=epochs, workers=cpu_count(), \n",
    "#     window=10, min_count=2, alpha=1e-2, min_alpha=1e-4,\n",
    "#     hs=1, negative=7, dbow_words=1, sample=0\n",
    "# )\n",
    "# d2v = Doc2Vec(**common_kwargs)\n",
    "# print(\"Building vocab...\")\n",
    "# d2v.build_vocab(vocab)\n",
    "# print(\"Training...\")\n",
    "# d2v.train(\n",
    "#     documents_d2c,\n",
    "#     total_examples=d2v.corpus_count,\n",
    "#     epochs=d2v.epochs\n",
    "# )\n",
    "# print(\"Finish...\")\n",
    "\n",
    "\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "\n",
    "\n",
    "common_kwargs = dict(\n",
    "    dm=0, vector_size=dim, epochs=epochs, workers=cpu_count(), \n",
    "    window=10, min_count=2, alpha=1e-2, min_alpha=1e-4,\n",
    "    hs=1, negative=7, dbow_words=1, sample=0\n",
    ")\n",
    "dbow = Doc2Vec(**common_kwargs)\n",
    "print(\"DBOW: Building vocab...\")\n",
    "dbow.build_vocab(vocab)\n",
    "print(\"DBOW: Training...\")\n",
    "dbow.train(\n",
    "    documents_d2c,\n",
    "    total_examples=dbow.corpus_count,\n",
    "    epochs=dbow.epochs\n",
    ")\n",
    "\n",
    "common_kwargs = dict(\n",
    "    dm=1, vector_size=dim, epochs=epochs, workers=cpu_count(), \n",
    "    window=10, min_count=2, alpha=1e-2, min_alpha=1e-4,\n",
    "    hs=1, negative=7, dbow_words=0, sample=0\n",
    ")\n",
    "dmm = Doc2Vec(**common_kwargs)\n",
    "print(\"DMM: Building vocab...\")\n",
    "dmm.build_vocab(vocab)\n",
    "print(\"DMM: Training...\")\n",
    "dmm.train(\n",
    "    documents_d2c,\n",
    "    total_examples=dmm.corpus_count,\n",
    "    epochs=dmm.epochs\n",
    ")\n",
    "\n",
    "d2v = ConcatenatedDoc2Vec([dbow, dmm])\n",
    "\n",
    "print(\"Finish...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = d2v.docvecs.vectors_docs\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3074, 1500)\n"
     ]
    }
   ],
   "source": [
    "def get_doc_vector(doc):\n",
    "    return d2v.docvecs[doc.tags[0]]\n",
    "\n",
    "X = []\n",
    "with ProcessPoolExecutor(max_workers=cpu_count()) as exc:\n",
    "    X = list(exc.map(get_doc_vector, documents_d2c, chunksize=100))\n",
    "X = np.asarray(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Elbow...\n",
      "Running Silhouette...\n"
     ]
    }
   ],
   "source": [
    "def calculate_silhouette_metric(X, K):\n",
    "    km_ = KMeansClusterer(\n",
    "        K, distance=cosine_distance,\n",
    "        repeats=10,\n",
    "        avoid_empty_clusters=True\n",
    "    )\n",
    "    assigned_clusters_ = np.array(km_.cluster(X, assign_clusters=True)).ravel()\n",
    "    centroids = km_.means()\n",
    "    sh_score = silhouette_score(X, assigned_clusters_, metric='cosine', random_state=0)\n",
    "    return [len(centroids), sh_score]\n",
    "\n",
    "print(\"Running Elbow...\")\n",
    "elbow_error = []\n",
    "elbow_amount_clusters = 2\n",
    "elbow_cosine_distance = np.copy(1 - cosine_similarity(X))\n",
    "for _ in range(5):\n",
    "    elbow_instance = elbow(elbow_cosine_distance, kmin, kmax + 1)\n",
    "    elbow_instance.process()\n",
    "    amc = elbow_instance.get_amount()\n",
    "    if amc > elbow_amount_clusters:\n",
    "        elbow_amount_clusters = amc\n",
    "        elbow_error = elbow_instance.get_wce()\n",
    "\n",
    "print(\"Running Silhouette...\")\n",
    "\n",
    "scores = []\n",
    "silhouette_amount_clusters = 0\n",
    "with ProcessPoolExecutor(max_workers=cpu_count()) as exc:\n",
    "    silhouette = np.asarray(list(exc.map(partial(calculate_silhouette_metric, X), range(kmin, kmax + 1), chunksize=5)))\n",
    "    amount, silhouette_scores = silhouette[:, 0], silhouette[:, 1]\n",
    "    silhouette_scores[np.isnan(silhouette_scores)] = -1\n",
    "    silhouette_amount_clusters = int(amount[np.where(silhouette_scores == np.max(silhouette_scores))][0])\n",
    "\n",
    "print(\"N. Elbow      Cluster : \", elbow_amount_clusters)\n",
    "print(\"N. Silhouette Cluster : \", silhouette_amount_clusters)\n",
    "qtd_cluster = elbow_amount_clusters if elbow_amount_clusters > silhouette_amount_clusters else silhouette_amount_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(elbow_error)), elbow_error)\n",
    "plt.xticks(range(len(elbow_error)), range(kmin, kmax + 1, 1))\n",
    "plt.xlabel(\"Qtd. Cluster\")\n",
    "plt.ylabel(\"Elbow (WCE)\")\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.plot(range(len(silhouette_scores)), silhouette_scores)\n",
    "plt.xticks(range(len(silhouette_scores)), range(kmin, kmax + 1, 1))\n",
    "plt.xlabel(\"Qtd. Cluster\")\n",
    "plt.ylabel(\"Silhouette\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclusterer = KMeansClusterer(\n",
    "    qtd_cluster,\n",
    "    distance=cosine_distance,\n",
    "    repeats=30,\n",
    "    avoid_empty_clusters=True\n",
    ")\n",
    "assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "assigned_clusters = np.array(assigned_clusters).ravel()\n",
    "centroids = kclusterer.means()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.asarray(df_data[\"title\"])\n",
    "\n",
    "info = []\n",
    "frases = []\n",
    "for i in range(qtd_cluster):\n",
    "    idx = np.where(assigned_clusters == i)[0]\n",
    "    for doc in docs[idx]:\n",
    "        frases.append([doc, i])\n",
    "    info.append([i, len(docs[idx])])\n",
    "\n",
    "df = pd.DataFrame(frases, columns=[\"title\", \"cluster\"])\n",
    "df_inf = pd.DataFrame(np.c_[[f\"C{i}\" for i in range(qtd_cluster)], np.asarray(info)[:, 1]])\n",
    "\n",
    "print(\"Documentos por cluster:\")\n",
    "data = []\n",
    "for i in range(qtd_cluster):\n",
    "    T = df.to_numpy()\n",
    "    data += utils.shuffle(T[T[:, 1] == i], n_samples=3).tolist()\n",
    "df = pd.DataFrame(data, columns=[\"title\", \"cluster\"])\n",
    "display(HTML(df.sort_values(by=[\"cluster\"]).to_html(index=False)))\n",
    "\n",
    "print(\"-\" * 20)\n",
    "print(\"Qtde documentos por cluster:\")\n",
    "HTML(df_inf.to_html(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
